{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from standard_functions import *\n",
    "path = os.getcwd()\n",
    "data_path = os.path.join(os.path.dirname(path), 'Data') \n",
    "\n",
    "BloombergData = pd.read_csv(data_path + \"/BloombergData_Swap_Features.csv\")\n",
    "preData = pd.read_csv(data_path + \"/TestData_Swap_Features_pre.csv\")\n",
    "postData = pd.read_csv(data_path + \"/TestData_Swap_Features_post.csv\")\n",
    "\n",
    "insample = np.array(BloombergData.iloc[:,2:].reset_index(drop=True)) \n",
    "insample_scaled = [x/100 for x in insample]\n",
    "insample_tensor = torch.from_numpy(np.float32(insample_scaled))\n",
    "\n",
    "X_train1, X_val1 = train_validation_split(BloombergData.reset_index(drop=True), 0.1) \n",
    "X_train2  = np.array(X_train1.iloc[:,2:].reset_index(drop=True)) \n",
    "X_train_scaled = [x/100 for x in X_train2]\n",
    "X_train_tensor = torch.from_numpy(np.float32(X_train_scaled))\n",
    "\n",
    "X_val2  = np.array(X_val1.iloc[:,2:].reset_index(drop=True)) \n",
    "X_val_scaled = [x/100 for x in X_val2]\n",
    "X_val_tensor = torch.from_numpy(np.float32(X_val_scaled))\n",
    "\n",
    "preX_test2  = np.array(preData.iloc[:,2:].reset_index(drop=True)) \n",
    "preX_test_scaled = [x/100 for x in preX_test2]\n",
    "preX_test_tensor = torch.from_numpy(np.float32(preX_test_scaled))\n",
    "\n",
    "postX_test2  = np.array(postData.iloc[:,2:].reset_index(drop=True)) \n",
    "postX_test_scaled = [x/100 for x in postX_test2]\n",
    "postX_test_tensor = torch.from_numpy(np.float32(postX_test_scaled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_odesolution(G_output, sigma_1, sigma_2, sigma_3, rho12, rho13, rho23, mu, \n",
    "                        r, encoded_mat, N, model, plot = False):\n",
    "    sigma_long = build_sigma_matrix_3factor(sigma_1, sigma_2, sigma_3, rho12, rho13, rho23) \n",
    "    mu_long = mu.repeat_interleave(30, dim=0)\n",
    "    G_long = G_output.reshape(-1, 1)\n",
    "    r_long = r.repeat_interleave(30, dim=0)\n",
    "    \n",
    "    alpha = reshape_wide(alpha_fct_3factor(encoded_mat, model, mu_long, sigma_long, G_long), N)\n",
    "    beta = reshape_wide(beta_fct(r_long, G_long), N)\n",
    "    gamma = reshape_wide(gamma_fct_3factor(encoded_mat, model, sigma_long), N)\n",
    "    \n",
    "    maturities = torch.arange(1, 31, dtype=torch.float32)\n",
    "\n",
    "    A, B = solve_ODE_constant(alpha, beta, gamma, maturities)\n",
    "    \n",
    "    p = torch.exp(torch.clamp(A - B * G_output, min = -80, max = 80))  # (N,30)\n",
    "    #p = torch.exp(A - B * G_output)\n",
    "    p_cumsum = torch.cumsum(p, dim=1)  # (N,30)\n",
    "    \n",
    "    s = (1 - p) / p_cumsum\n",
    "    \n",
    "    # Variance \n",
    "    row_std_devalpha = alpha.std(dim=1).detach().sum()\n",
    "    row_std_devbeta = beta.std(dim=1).detach().sum()\n",
    "    row_std_devgamma = gamma.std(dim=1).detach().sum()\n",
    "    row_std_devA = A.std(dim=1).detach().sum()\n",
    "    row_std_devB = B.std(dim=1).detach().sum()\n",
    "\n",
    "\n",
    "    # if plot: \n",
    "    #     plot_all(alpha, beta, gamma, A, B)\n",
    "\n",
    "    check_nan_inf(p, \"P\")\n",
    "    check_nan_inf(B, \"B\")\n",
    "    check_nan_inf(A, \"A\")\n",
    "    check_nan_inf(G_output, \"G_output\")\n",
    "    check_nan_inf(alpha, \"alpha\")\n",
    "    check_nan_inf(beta, \"beta\")\n",
    "    check_nan_inf(gamma, \"gamma\")\n",
    "    check_nan_inf(s, \"s\")\n",
    "\n",
    "    # Arbitrage loss\n",
    "    dA = B**2 * gamma \n",
    "    dB = B * alpha + beta\n",
    "\n",
    "    # if plot:\n",
    "        # plot_all(alpha, beta, gamma, A, B, datapoint=0)\n",
    "        # plot_constant(A, B, dA, dB, alpha, beta, gamma, datapoint=0)\n",
    "\n",
    "   \n",
    "    arb_l = arb_equation_3factor(G_output, sigma_1, sigma_2, sigma_3, rho12, rho13, rho23, mu, r, encoded_mat, model, dA, dB, B)\n",
    "\n",
    "    return s, p, arb_l, alpha, beta, gamma, A, B, row_std_devalpha, row_std_devbeta, row_std_devgamma, row_std_devA, row_std_devB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(8, 3, bias = False)\n",
    "        )\n",
    "\n",
    "        self.sigma = nn.Sequential(              \n",
    "            nn.Linear(3, 10, bias = False),\n",
    "            centered_softmax(),   \n",
    "            nn.Linear(10, 6, bias = False)\n",
    "        )\n",
    "\n",
    "        self.mu = nn.Sequential(\n",
    "            nn.Linear(3, 3, bias = True)\n",
    "        )\n",
    "\n",
    "        self.r = nn.Sequential(\n",
    "            nn.Linear(3, 4, bias = False),\n",
    "            centered_softmax(),\n",
    "            nn.Linear(4, 1, bias = False)    \n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(4, 10, bias = False),\n",
    "            centered_softmax(),\n",
    "            nn.Linear(10, 1, bias = False)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                # if m.bias is not None: \n",
    "                #     nn.init.xavier_uniform_(m.bias) # maybe works?\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Encoder\n",
    "        encoded = self.encoder(x) \n",
    "        \n",
    "        # Maturity \n",
    "        mat = torch.arange(1, 31, dtype=torch.float32).repeat(len(encoded)) # length N*30 array of 1-30,1-30 and so on N times - range(1,31)*N\n",
    "        x1 = encoded.repeat_interleave(30, dim=0)                           # N*30x2 \n",
    "        encoded_mat = torch.cat([x1, mat.unsqueeze(1)], dim=1)              # N*30x3\n",
    "\n",
    "        # Decoder --> gives y for all maturities \n",
    "        G_output = self.decoder(encoded_mat).reshape(len(x), 30)\n",
    "\n",
    "        # H network for sigma \n",
    "        sigma_1, sigma_2, sigma_3, rho12, rho13, rho23 = torch.split(self.sigma(encoded), 1, dim=1)\n",
    "\n",
    "        # K network for mu\n",
    "        mu = self.mu(encoded)\n",
    "\n",
    "        # R network for rate \n",
    "        r = self.r(encoded)\n",
    "\n",
    "        return G_output, sigma_1, sigma_2, sigma_3, rho12, rho13, rho23, mu, r, encoded_mat, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(4)\n",
    "\n",
    "# model = Autoencoder()   \n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "# lr_sched = LR_Scheduler(optimizer, percentage = 0.9, interval = 50)\n",
    "\n",
    "# # Training\n",
    "# num_epochs = 3597\n",
    "# batch_size = 32\n",
    "# data_loader = torch.utils.data.DataLoader(X_train_tensor, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# swap_mats = [1, 2, 3, 5, 10, 15, 20, 30]\n",
    "# std_alp_epoch, std_bet_epoch, std_gam_epoch, std_A_epoch, std_B_epoch, arb_losses_list, losses_list, vallosses_list = train_ae3(forward_odesolution, X_train_tensor, X_val_tensor, swap_mats, num_epochs, data_loader, model, criterion, optimizer, lr_sched = lr_sched)\n",
    "\n",
    "# print(std_alp_epoch)\n",
    "# print(std_bet_epoch)\n",
    "# print(std_gam_epoch)\n",
    "# print(std_A_epoch)\n",
    "# print(std_B_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(\"models\", exist_ok=True)\n",
    "name = \"3factor_AF_constant\"\n",
    "# torch.save(model.state_dict(), f\"models/{name}.pth\")\n",
    "# print(f\"Model saved successfully as models/{name}.pth\")\n",
    "model2 = Autoencoder()       # Ensure Autoencoder class is defined\n",
    "model2.load_state_dict(torch.load(f\"models/{name}.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
