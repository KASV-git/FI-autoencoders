{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd.functional import hessian\n",
    "\n",
    "\n",
    "from torch.func import jacfwd \n",
    "from torch.func import vmap \n",
    "\n",
    "from torchdiffeq import odeint\n",
    "from standard_functions import LR_Scheduler, train_validation_split, plot_all\n",
    "\n",
    "path = os.getcwd()\n",
    "data_path = os.path.join(os.path.dirname(path), 'Data') \n",
    "\n",
    "BloombergData = pd.read_csv(data_path + \"/BloombergData_Swap_Features.csv\")\n",
    "preData = pd.read_csv(data_path + \"/TestData_Swap_Features_pre.csv\")\n",
    "postData = pd.read_csv(data_path + \"/TestData_Swap_Features_post.csv\")\n",
    "\n",
    "insample = np.array(BloombergData.iloc[:,2:].reset_index(drop=True)) \n",
    "insample_scaled = [x/100 for x in insample]\n",
    "insample_tensor = torch.from_numpy(np.float32(insample_scaled))\n",
    "\n",
    "X_train1, X_val1 = train_validation_split(BloombergData.reset_index(drop=True), 0.1) \n",
    "X_train2  = np.array(X_train1.iloc[:,2:].reset_index(drop=True)) \n",
    "X_train_scaled = [x/100 for x in X_train2]\n",
    "X_train_tensor = torch.from_numpy(np.float32(X_train_scaled))\n",
    "\n",
    "X_val2  = np.array(X_val1.iloc[:,2:].reset_index(drop=True)) \n",
    "X_val_scaled = [x/100 for x in X_val2]\n",
    "X_val_tensor = torch.from_numpy(np.float32(X_val_scaled))\n",
    "\n",
    "preX_test2  = np.array(preData.iloc[:,2:].reset_index(drop=True)) \n",
    "preX_test_scaled = [x/100 for x in preX_test2]\n",
    "preX_test_tensor = torch.from_numpy(np.float32(preX_test_scaled))\n",
    "\n",
    "postX_test2  = np.array(postData.iloc[:,2:].reset_index(drop=True)) \n",
    "postX_test_scaled = [x/100 for x in postX_test2]\n",
    "postX_test_tensor = torch.from_numpy(np.float32(postX_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_centered_softmax(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + torch.exp(-x)) - 0.5\n",
    "    \n",
    "def grad_latent_2factor(res, model):\n",
    "\n",
    "    calc_grad_G = vmap(jacfwd(model.decoder, argnums=0), in_dims=(0)) \n",
    "    grad_G = calc_grad_G(res).squeeze(dim=1) # N*30X3\n",
    "\n",
    "    gradG_dz   = grad_G[:, :2] # N*30X2\n",
    "    dG_dm      = grad_G[:, 2:] # N*30X1\n",
    "\n",
    "    return gradG_dz, dG_dm\n",
    "\n",
    "def hess_latent_2factor(res, model):\n",
    "    \n",
    "    calc_hess_G = vmap(jacfwd(jacfwd(model.decoder, argnums=0), argnums=0), in_dims=(0))\n",
    "    hess_G = calc_hess_G(res).squeeze(dim=1)\n",
    "\n",
    "    hessG_dz = hess_G[:,:2,:2]\n",
    "    \n",
    "    return hessG_dz\n",
    "\n",
    "def alpha_fct(res, model, mu, sigma, G):\n",
    "    gradG_dz, dG_dm = grad_latent_2factor(res, model)\n",
    "    hessG_dz = hess_latent_2factor(res, model)\n",
    "\n",
    "    part1 = -dG_dm\n",
    "\n",
    "    part2 = torch.matmul(gradG_dz.unsqueeze(1), mu.unsqueeze(2))#.detach()\n",
    "    part2 = part2.squeeze(-1)\n",
    "\n",
    "    part0 = torch.matmul(sigma.transpose(1, 2), torch.matmul(hessG_dz, sigma))#.detach()\n",
    "\n",
    "    part3 = 0.5 * torch.einsum('bii->b', part0).unsqueeze(-1)\n",
    "\n",
    "    return (part1 + part2 + part3) / G\n",
    "\n",
    "def beta_fct(r,G):\n",
    "    return r/G\n",
    "\n",
    "def gamma_fct(res, model, sigma):\n",
    "    gradG_dz, _ = grad_latent_2factor(res, model)\n",
    "\n",
    "    grad_grad = torch.matmul(gradG_dz.unsqueeze(-1), gradG_dz.unsqueeze(-1).transpose(1, 2))\n",
    "    part0 = torch.matmul(sigma.transpose(1, 2), grad_grad.matmul(sigma))\n",
    "\n",
    "    return 0.5 * torch.einsum('bii->b', part0).unsqueeze(-1)\n",
    "\n",
    "def build_sigma_matrix_2factor(sigma_1, sigma_2, rho):\n",
    "    sigma_matrix = torch.zeros((len(sigma_1), 2, 2))  \n",
    "    sigma_matrix[:, 0, 0] = torch.exp(sigma_1.squeeze(-1))\n",
    "    sigma_matrix[:, 0, 1] = 0  \n",
    "    sigma_matrix[:, 1, 0] = torch.tanh(rho.squeeze(-1)) * torch.exp(sigma_2.squeeze(-1))\n",
    "    sigma_matrix[:, 1, 1] = torch.sqrt(1 - torch.tanh(rho.squeeze(-1))**2) * torch.exp(sigma_2.squeeze(-1))\n",
    "\n",
    "    return sigma_matrix.repeat_interleave(30, dim=0)\n",
    "\n",
    "class ODESystem(torch.nn.Module):\n",
    "    def __init__(self, alpha, beta, gamma):\n",
    "        super(ODESystem, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, t, x):\n",
    "\n",
    "        alpha_t = piecewise_constant_param(t, self.alpha)\n",
    "        beta_t = piecewise_constant_param(t, self.beta)\n",
    "        gamma_t = piecewise_constant_param(t, self.gamma)\n",
    "\n",
    "        dxdt = torch.zeros_like(x)\n",
    "        \n",
    "        dxdt[:, 0] = gamma_t * x[:, 1]**2\n",
    "        dxdt[:, 1] = alpha_t * x[:, 1] + beta_t\n",
    "        \n",
    "        return dxdt\n",
    "    \n",
    "def reshape_wide(x, length):\n",
    "    x = x.reshape(length, 30)\n",
    "\n",
    "    return x\n",
    "\n",
    "def piecewise_constant_param(t, param_table):\n",
    "    if not isinstance(t, torch.Tensor):\n",
    "        t = torch.tensor(t, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    t_clamped = torch.clamp(t, min=0, max=30)\n",
    "\n",
    "    idx = torch.ceil(t_clamped).detach() + t_clamped - t_clamped.detach() - 1\n",
    "    idx = idx.long() \n",
    "\n",
    "    param1 = param_table[:, idx]\n",
    "\n",
    "    return param1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arb_equation_2factor(G, sigma_1, sigma_2, rho , mu, r, encoded_mat, model, dA, dB, B):\n",
    "    N = len(G)\n",
    "    \n",
    "    r_long = r.repeat_interleave(30, dim=0) \n",
    "    G_long = G.reshape(N*30, 1)\n",
    "    dA_long = dA.reshape(N*30, 1)\n",
    "    dB_long = dB.reshape(N*30, 1)\n",
    "    B_long = B.reshape(N*30, 1)\n",
    "    mu_long = mu.repeat_interleave(30, dim=0)      \n",
    "    grad_z, dy_dm   = grad_latent_2factor(encoded_mat, model)  \n",
    "    \n",
    "    \n",
    "    grad_zmu = torch.matmul(grad_z.unsqueeze(1), mu_long.unsqueeze(2)).squeeze(-1) \n",
    "    \n",
    "    hess_z  = hess_latent_2factor(encoded_mat, model) \n",
    "    sigma_long = build_sigma_matrix_2factor(sigma_1, sigma_2, rho)\n",
    "    sigma_hess_sigma = torch.matmul(sigma_long.transpose(1, 2), torch.matmul(hess_z, sigma_long)) \n",
    "    trace_hess = 0.5 * torch.einsum('bii->b', sigma_hess_sigma).unsqueeze(-1)\n",
    "    \n",
    "    grad_grad = torch.matmul(grad_z.unsqueeze(-1), grad_z.unsqueeze(-1).transpose(1, 2))\n",
    "    sigma_grad_grad_sigma = torch.matmul(sigma_long.transpose(1, 2), torch.matmul(grad_grad, sigma_long)) \n",
    "    trace_grad_grad = 0.5 * torch.einsum('bii->b', sigma_grad_grad_sigma).unsqueeze(-1)\n",
    "    \n",
    "    final_term = -r_long - dA_long + G_long*dB_long + B_long*(dy_dm - grad_zmu - trace_hess) + (B_long**2)*trace_grad_grad   \n",
    "    \n",
    "    return final_term\n",
    "\n",
    "def reconstruct(G_output, sigma_1, sigma_2, rho, mu, r, encoded_mat, N, model):\n",
    "    sigma_long = build_sigma_matrix_2factor(sigma_1, sigma_2, rho)  # all N*30 long\n",
    "    mu_long = mu.repeat_interleave(30, dim=0)\n",
    "    G_long = G_output.reshape(-1, 1)\n",
    "    r_long = r.repeat_interleave(30, dim=0)\n",
    "    \n",
    "    \n",
    "    alpha = reshape_wide(alpha_fct(encoded_mat, model, mu_long, sigma_long, G_long), N)\n",
    "    beta = reshape_wide(beta_fct(r_long, G_long), N)\n",
    "    gamma = reshape_wide(gamma_fct(encoded_mat, model, sigma_long), N)\n",
    "    \n",
    "    X_0 = torch.zeros(N, 2)\n",
    "\n",
    "    t_span = torch.arange(0, 31, dtype=torch.float32) \n",
    "    ode_system = ODESystem(alpha, beta, gamma)\n",
    "    dX_sol = odeint(ode_system, X_0, t_span, method='rk4')\n",
    "    dX_sol = dX_sol.permute(1, 0, 2)\n",
    "    A = dX_sol[:, 1:, 0] # shape (N,)\n",
    "    B = dX_sol[:, 1:, 1]  # shape (N,)\n",
    "\n",
    "    p = torch.exp(torch.clamp((A-B*G_output), min = -80, max = 80)) # N,30\n",
    "    \n",
    "    p_cumsum = torch.cumsum(p, axis=1) # N,30\n",
    "    \n",
    "    s = (1-p)/p_cumsum # N,30\n",
    "    \n",
    "    p_long = p.reshape(N*30,1)\n",
    "    \n",
    "    # Arbitrage loss\n",
    "    dA = B**2 * gamma \n",
    "    dB = B * alpha + beta\n",
    "   \n",
    "    arb_l = arb_equation_2factor(G_output, sigma_1, sigma_2, rho, mu, r, encoded_mat, model, dA, dB, B)\n",
    "\n",
    "    plot_all(alpha, beta, gamma, A, B)\n",
    "    \n",
    "\n",
    "    return s, arb_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(8, 2, bias = False)\n",
    "        )\n",
    "\n",
    "        self.sigma = nn.Sequential(                \n",
    "            nn.Linear(2, 4, bias = False),\n",
    "            nn_centered_softmax(), \n",
    "            nn.Linear(4, 3, bias = False)\n",
    "        )\n",
    "\n",
    "        self.mu = nn.Sequential(\n",
    "            nn.Linear(2, 2, bias = True)\n",
    "        )\n",
    "\n",
    "        self.r = nn.Sequential(\n",
    "            nn.Linear(2, 4, bias = False),\n",
    "            nn_centered_softmax(),\n",
    "            nn.Linear(4, 1, bias = False)    \n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 10, bias = False),\n",
    "            nn_centered_softmax(),\n",
    "            nn.Linear(10, 1, bias = False)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                # if m.bias is not None: \n",
    "                #     nn.init.xavier_uniform_(m.bias) # maybe works?\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        encoded = self.encoder(x) \n",
    "    \n",
    "        mat = torch.arange(1, 31, dtype=torch.float32).repeat(len(encoded)) # length N*30 array of 1-30,1-30 and so on N times - range(1,31)*N\n",
    "        x1 = encoded.repeat_interleave(30, dim=0)                           # N*30x2 \n",
    "        encoded_mat = torch.cat([x1, mat.unsqueeze(1)], dim=1)              # N*30x3\n",
    "\n",
    "        # Decoder --> gives y for all maturities \n",
    "        G_output = self.decoder(encoded_mat).reshape(len(x), 30)\n",
    "\n",
    "        # K network for sigma \n",
    "        sigma_1, sigma_2, rho = torch.split(self.sigma(encoded), 1, dim=1)\n",
    "\n",
    "        # H network for mu\n",
    "        mu = self.mu(encoded)\n",
    "\n",
    "        # R network for rate \n",
    "        r = self.r(encoded)\n",
    "\n",
    "        return G_output, sigma_1, sigma_2, rho, mu, r, encoded_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.manual_seed(1)\n",
    "\n",
    "# model = Autoencoder()   \n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "# scheduler = LR_Scheduler(optimizer, percentage=0.9, interval = 50)\n",
    "\n",
    "\n",
    "# # Training\n",
    "# num_epochs = 3496\n",
    "# batch_size = 32\n",
    "# data_loader = torch.utils.data.DataLoader(X_train_tensor, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# swap_mats = [1, 2, 3, 5, 10, 15, 20, 30]\n",
    "# swap_mats0 = [i-1 for i in swap_mats]\n",
    "# arb_losses_list = []\n",
    "# losses_list = []\n",
    "# vallosses_list = []\n",
    "# for epoch in range(num_epochs):\n",
    "#     arb_loss_epoch = 0\n",
    "#     loss_epoch = 0\n",
    "#     for batch in data_loader:\n",
    "\n",
    "#         N = len(batch)\n",
    "\n",
    "#         G_output, sigma_1, sigma_2, rho, mu, r, encoded_mat = model(batch)\n",
    "\n",
    "#         reconstructed, arb_loss = reconstruct(G_output, sigma_1, sigma_2, rho, mu, r, encoded_mat, N, model)\n",
    "#         #print(reconstructed.shape)\n",
    "#         s_final = reconstructed[:, swap_mats0]\n",
    "#         #print(s_final)\n",
    "#         loss = criterion(s_final, batch)\n",
    "        \n",
    "#         # Backward pass and optimization\n",
    "\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward(retain_graph=True)\n",
    "\n",
    "#         nan_detected = False\n",
    "#         for param in model.parameters():\n",
    "#             if param.grad is not None and torch.isnan(param.grad).any():\n",
    "#                     print(f\"NaN detected in gradients at epoch {epoch}, resetting to zero.\")\n",
    "#                     param.grad.zero_() \n",
    "#                     nan_detected = True\n",
    "\n",
    "#         if not nan_detected:\n",
    "#                 optimizer.step()\n",
    "#         else:\n",
    "#                 print(f\"Skipping optimizer update at epoch {epoch} due to NaNs.\")\n",
    "\n",
    "#         #arb_losses_list.append(arb_loss)\n",
    "#         arb_loss_epoch += torch.sum(arb_loss)\n",
    "#         loss_epoch += loss\n",
    "\n",
    "#      #\n",
    "#     N = len(X_val_tensor)    \n",
    "#     G_output, sigma_1, sigma_2, rho, mu, r, encoded_mat = model(X_val_tensor)\n",
    "#     reconstructed, arb_loss  = reconstruct(G_output, sigma_1, sigma_2, rho, mu, r, encoded_mat, N, model)\n",
    "#     s_final = reconstructed[:, swap_mats0]\n",
    "#     loss_val = criterion(s_final, X_val_tensor)\n",
    "\n",
    "#     #\n",
    "#     N = len(X_train_tensor)    \n",
    "#     G_output, sigma_1, sigma_2, rho, mu, r, encoded_mat = model(X_train_tensor)\n",
    "#     reconstructed, arb_loss  = reconstruct(G_output, sigma_1, sigma_2, rho, mu, r, encoded_mat, N, model)\n",
    "#     s_final = reconstructed[:, swap_mats0]\n",
    "#     loss_train = criterion(s_final, X_train_tensor)\n",
    "\n",
    "#     # append relevant data\n",
    "#     vallosses_list.append(loss_val.item())\n",
    "#     arb_losses_list.append(arb_loss_epoch.item())\n",
    "#     losses_list.append(loss_train.item())\n",
    "\n",
    "#     scheduler.step()\n",
    "\n",
    "#     if (epoch + 1) % 100 == 0:\n",
    "#         print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss_train.item():.8f}, Arbitrage loss: {(arb_loss_epoch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # os.makedirs(\"models\", exist_ok=True)\n",
    "name = \"2factor_AF_odeint\"\n",
    "# torch.save(model.state_dict(), f\"models/{name}.pth\")\n",
    "# print(f\"Model saved successfully as models/{name}.pth\")\n",
    "model2 = Autoencoder()       # Ensure Autoencoder class is defined\n",
    "model2.load_state_dict(torch.load(f\"models/{name}.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
