{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from standard_functions import *\n",
    "path = os.getcwd()\n",
    "data_path = os.path.join(os.path.dirname(path), 'Data') \n",
    "\n",
    "BloombergData = pd.read_csv(data_path + \"/BloombergData_Swap_Features.csv\")\n",
    "preData = pd.read_csv(data_path + \"/TestData_Swap_Features_pre.csv\")\n",
    "postData = pd.read_csv(data_path + \"/TestData_Swap_Features_post.csv\")\n",
    "\n",
    "insample = np.array(BloombergData.iloc[:,2:].reset_index(drop=True)) \n",
    "insample_scaled = [x/100 for x in insample]\n",
    "insample_tensor = torch.from_numpy(np.float32(insample_scaled))\n",
    "\n",
    "X_train1, X_val1 = train_validation_split(BloombergData.reset_index(drop=True), 0.1) \n",
    "X_train2  = np.array(X_train1.iloc[:,2:].reset_index(drop=True)) \n",
    "X_train_scaled = [x/100 for x in X_train2]\n",
    "X_train_tensor = torch.from_numpy(np.float32(X_train_scaled))\n",
    "\n",
    "X_val2  = np.array(X_val1.iloc[:,2:].reset_index(drop=True)) \n",
    "X_val_scaled = [x/100 for x in X_val2]\n",
    "X_val_tensor = torch.from_numpy(np.float32(X_val_scaled))\n",
    "\n",
    "preX_test2  = np.array(preData.iloc[:,2:].reset_index(drop=True)) \n",
    "preX_test_scaled = [x/100 for x in preX_test2]\n",
    "preX_test_tensor = torch.from_numpy(np.float32(preX_test_scaled))\n",
    "\n",
    "postX_test2  = np.array(postData.iloc[:,2:].reset_index(drop=True)) \n",
    "postX_test_scaled = [x/100 for x in postX_test2]\n",
    "postX_test_tensor = torch.from_numpy(np.float32(postX_test_scaled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, yhat, y, L, w):  \n",
    "        mse = torch.mean(torch.sum(torch.pow(yhat - y, 2),1)/8)\n",
    "        arbitrage_loss = torch.mean(torch.sum(torch.pow(L, 2), 1)/30)\n",
    "       \n",
    "        return mse + (w*arbitrage_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(8, 3, bias = False)\n",
    "        )\n",
    "        \n",
    "        self.sigma = nn.Sequential(\n",
    "            nn.Linear(3, 6, bias = False),\n",
    "            centered_softmax(),                 \n",
    "            nn.Linear(6, 6, bias = False)\n",
    "        )\n",
    "\n",
    "        self.mu = nn.Sequential(\n",
    "            nn.Linear(3, 3, bias = False),\n",
    "            centered_softmax(),\n",
    "            nn.Linear(3, 3, bias = False)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(4, 10, bias = False),\n",
    "            centered_softmax(),\n",
    "            nn.Linear(10, 1, bias = False)\n",
    "        )\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        encoded = self.encoder(x) \n",
    "        \n",
    "        mat = torch.arange(1, 31, dtype=torch.float32).repeat(len(encoded))\n",
    "        \n",
    "        # y for 1-30\n",
    "        x1 = encoded.repeat_interleave(30, dim=0)\n",
    "        encoded_mat = torch.cat([x1, mat.unsqueeze(1)], dim=1)     \n",
    "        y = self.decoder(encoded_mat).reshape(len(encoded), 30)\n",
    "\n",
    "        # r i.e y[:,0]\n",
    "        mat0 = torch.zeros(len(encoded),1)\n",
    "        encoded_mat_0 = torch.cat([encoded, mat0], dim=1) \n",
    "        r = self.decoder(encoded_mat_0)\n",
    "        \n",
    "        sigma_1, sigma_2, sigma_3, rho12, rho13, rho23 = torch.split(self.sigma(encoded), 1, dim=1)\n",
    "        # K network for mu\n",
    "        mu = self.mu(encoded)\n",
    "\n",
    "        #Remove explosion in the first 1-5 epochs. Tau 30 + bad weight ini cause P > 1000. Training gets stuck. \n",
    "        p = torch.exp(torch.clamp((-mat.reshape(y.shape[0],30) * y), min = -80, max = 80))\n",
    "        \n",
    "        p_cumsum = torch.cumsum(p, dim=1)  \n",
    "\n",
    "        s = (1 - p) / p_cumsum\n",
    "        \n",
    "        return r, y, p, sigma_1, sigma_2, sigma_3, rho12, rho13, rho23, mu, encoded, encoded_mat, s, mat    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arb_loss(r, y, p, sigma_1, sigma_2, sigma_3, rho12, rho13, rho23, mu, encoded_mat, tau, model):\n",
    "    N = len(y)\n",
    "    \n",
    "    r = r.reshape(N, 1)                  \n",
    "    r_long = r.repeat_interleave(30, dim=0) \n",
    "    y_long = y.reshape(N*30, 1)\n",
    "    tau_long =tau.reshape(N*30, 1) \n",
    "    mu_long = mu.repeat_interleave(30, dim=0)     \n",
    "    \n",
    "    grad_zy, dy_dm   = grad_latent_3factor(encoded_mat, model) \n",
    "    \n",
    "    grad_zmu = torch.matmul(grad_zy.unsqueeze(1), mu_long.unsqueeze(2)).squeeze(-1)\n",
    "     \n",
    "    hess_z  = hess_latent_3factor(encoded_mat, model) \n",
    "    sigma_long = build_sigma_matrix_3factor(sigma_1, sigma_2, sigma_3, rho12, rho13, rho23)\n",
    "    sigma_hess_sigma = torch.matmul(sigma_long.transpose(1, 2), torch.matmul(hess_z, sigma_long)) \n",
    "    trace_hess = 0.5 * torch.einsum('bii->b', sigma_hess_sigma).unsqueeze(-1)\n",
    "    \n",
    "    grad_grad = torch.matmul(grad_zy.unsqueeze(-1), grad_zy.unsqueeze(-1).transpose(1, 2))\n",
    "    sigma_grad_grad_sigma = torch.matmul(sigma_long.transpose(1, 2), torch.matmul(grad_grad, sigma_long)) \n",
    "    trace_grad_grad = 0.5 * torch.einsum('bii->b', sigma_grad_grad_sigma).unsqueeze(-1)\n",
    "    \n",
    "    final_term =  -r_long + y_long + tau_long*(dy_dm - grad_zmu - trace_hess) + (tau_long**2)*trace_grad_grad   \n",
    "        \n",
    "    return final_term.reshape(len(y), 30)*p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRMSE(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, yhat, y):  \n",
    "        mse = torch.mean(torch.sum(torch.pow(yhat - y, 2),1)/8)\n",
    "\n",
    "        return mse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(3)\n",
    "\n",
    "# model = Autoencoder()   \n",
    "# criterion= CustomLoss()\n",
    "# rmse = CustomRMSE()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# scheduler = LR_Scheduler(optimizer, percentage=0.9, interval = 50)\n",
    "\n",
    "# # Training\n",
    "# num_epochs = 4993 \n",
    "# batch_size = 32\n",
    "# data_loader = torch.utils.data.DataLoader(X_train_tensor, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# swap_mats = [1, 2, 3, 5, 10, 15, 20, 30]\n",
    "# swap_mats0 = [i-1 for i in swap_mats]\n",
    "\n",
    "# arb_losses_list = []\n",
    "# losses_list = []\n",
    "# val_loss_list = []\n",
    "# losses_list_rmse = []\n",
    "# val_loss_list_rmse = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     arb_loss_epoch = 0\n",
    "#     loss_epoch = 0\n",
    "\n",
    "#     for batch in data_loader:\n",
    "        \n",
    "#         # Forward pass\n",
    "#         r, y, p, sigma_1, sigma_2, sigma_3, rho12, rho13, rho23, mu, encoded, encoded_mat, reconstructed, mat = model(batch)\n",
    "\n",
    "#         arbitrage_loss = arb_loss(r, y, p, sigma_1, sigma_2, sigma_3, rho12, rho13, rho23, mu, encoded_mat, mat, model)\n",
    "                                      \n",
    "#         s_final = reconstructed[:, swap_mats0]\n",
    "        \n",
    "#         loss = criterion(s_final, batch, arbitrage_loss, 1)\n",
    "        \n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         arb_loss_epoch += torch.sum(arbitrage_loss)\n",
    "#         loss_epoch += loss\n",
    "    \n",
    "#     r, y, p, sigma_1, sigma_2, sigma_3, rho12, rho13, rho23, mu, encoded, encoded_mat, reconstructed, mat = model(X_val_tensor)\n",
    "#     s_final = reconstructed[:, swap_mats0]\n",
    "#     arbitrage_loss = arb_loss(r, y, p, sigma_1, sigma_2, sigma_3, rho12, rho13, rho23, mu, encoded_mat, mat, model)\n",
    "#     loss_val = criterion(s_final, X_val_tensor, arbitrage_loss, 1)\n",
    "#     loss_val_rmse = rmse(s_final, X_val_tensor)\n",
    "\n",
    "#     r, y, p, sigma_1, sigma_2, sigma_3, rho12, rho13, rho23, mu, encoded, encoded_mat, reconstructed, mat = model(X_train_tensor)\n",
    "#     s_final = reconstructed[:, swap_mats0]\n",
    "#     arbitrage_loss = arb_loss(r, y, p, sigma_1, sigma_2, sigma_3, rho12, rho13, rho23, mu, encoded_mat, mat, model)\n",
    "#     loss_train = criterion(s_final, X_train_tensor, arbitrage_loss, 1)\n",
    "#     loss_train_rmse = rmse(s_final, X_train_tensor)\n",
    "\n",
    "#     losses_list.append(loss_train.item())\n",
    "#     losses_list_rmse.append(loss_train_rmse.item())\n",
    "#     val_loss_list.append(loss_val.item())\n",
    "#     val_loss_list_rmse.append(loss_val_rmse.item())   \n",
    "#     #arb_losses_list.append(arb_loss_epoch.item())\n",
    "    \n",
    "#     scheduler.step()\n",
    "#     current_lr = optimizer.param_groups[0][\"lr\"] \n",
    "    \n",
    "#     if (epoch + 1) % 100 == 0:\n",
    "#         print(f\"Epoch [{epoch + 1}/{num_epochs}], LR:{current_lr:.8f}, Loss: {loss_train.item():.8f}, Arb:{arb_loss_epoch:.8f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"3factor_minArb\"\n",
    "# torch.save(model.state_dict(), f\"models/{name}.pth\")\n",
    "# print(f\"Model saved successfully as models/{name}.pth\")\n",
    "model2 = Autoencoder()       # Ensure Autoencoder class is defined\n",
    "model2.load_state_dict(torch.load(f\"models/{name}.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
